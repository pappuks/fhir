# Example code to train a TensorFlow model from FHIR Bundles on Google Cloud Platform (GCP)

This directory contains example code to generate labels and training examples
for a tensorflow model, as well as for training the model.

We need to first create Cloud Datalab and Cloud Dataproc cluster in GCP.

Start by cloning this repository to your workstation/local machine.

This repository contains 2 types of resources for running this example on GCP:

1. [Notebooks](./notebooks "Jupyter Notebooks") contains jupyter notebooks for
various phases involved to take patient healthcare data, transform it and
eventually use it to train and evaluate a sample Machine Learning model.

2. [Scripts](./# "Scripts") to provision Cloud datalab and Cloud Dataproc
cluster in Google Cloud Platform(GCP)

Additionally, we have included scripts to add ingress firewall rules required
to open an ssh connection from your local workstation to Cloud Datalab running
on GCP. This will enable a Chrome web browser to securely access Cloud datalab
over SSH tunnel.

## Provision a Cloud Datalab on a Cloud Dataproc cluster in GCP

You will need a [GCP Project](https://cloud.google.com/storage/docs/projects).
Either you identify a preexisting project or create a new one using
[GCP console](https://cloud.google.com/compute/docs/console).

You will also need [Google Cloud SDK.](https://cloud.google.com/sdk/install)
[Initialize](https://cloud.google.com/sdk/docs/initializing) Google Cloud SDK
and select the GCP Project where you want to provision the cluster.

Update your env.sh file based on the sample provided in
[env.sh]( ./scripts/provisioning/env.sh) .

Source the environment file you've created. Be aware that the bucket name
needs to be unique. If the bucket already exists then the script will not fail
but other scripts may not work.

```bash
eg. source ./myenv.sh
```

Run the [01-prep.sh](./01-prep.sh) script to move the
[cluster initialization scripts](./init-scripts/) from your workstation to
Google cloud storage bucket
This script can also create a persistent hive metastore (CloudSQL instance).
If you do want to use an a hive metastore with this deployment
please pass in argument 'hivemeta'

If you get 'AccessDeniedException: 403" please make sure the bucket exist or
the bucket name specifed in your myenv.sh file is unique.

```
./01-prep.sh {hivemeta}
```

Run [02-gen-fhirdependencies.sh](./02-gen-fhirdependencies.sh) to build the fhir code
repositories. This needs to be done only as per need basis.

```
./02-gen-fhirdependencies.sh
```

Then run the [03-cluster.sh](./03-cluster.sh) to create a new dataproc cluster.
This will create a 3 node cluster in GCP. Cloud datalab will be provisioned on
the master node of this cluster. If you do want to use an a hive metastore
with this deployment please pass in argument 'hivemeta'

```
./03-cluster.sh {hivemeta}
```

We use synthetic data from [Synthea<sup>TM</sup>](https://syntheticmass.mitre.org/)
which is parsed into protocol buffers. Labels are generated from the bundles
for a length-of-stay task using an inpatient cohort. Samples are split into
train and validation datasets.

Some temporary files will be generated by this example, in the directory passed
as an argument to the scripts. Beware that some steps can take many minutes to
execute.

Run [04-create-training-data.sh](./04-create-training-data.sh) to create
training data(JSON FHIR bundles) using [
Synthea<sup>TM</sup>](https://github.com/synthetichealth/synthea). This script
will upload training data to GCS Bucket in your GCP project

```
./04-create-training-data.sh {scratch directory}
```

Then run the [05-gen-bundles.sh](./05-gen-bundles.sh) to generate training
dataset in TFRecord file format. This script converts training data created in
a previous step to TFRecord file format and uploads it to GCS Bucket in your
GCP project.

```
./05-gen-bundles.sh {scratch directory}
```

The above steps sets up a Cloud Datalab and Dataproc environment in GCP for Data
Analysis and Machine Learning. You can use following scripts to connect to Cloud
datalab instance using Chrome Web Browser

## Access Cloud Datalab over a secure SSH tunnel
Use the following helper scripts to access Cloud Datalab:

[firewall_ssh.sh](./firewall_ssh.sh) script adds an
ingress firewall rule required for establishing ssh tunnel between your
local machine and Cloud Datalab running on GCP.
```
./firewall_ssh.sh {your env file}
```

[sshtunnel.sh](./sshtunnel.sh) script opens up an
ssh tunnel to the master node. This will enable a Chrome web browser to
securely access Cloud datalab over SSH tunnel.
```
./sshtunnel.sh {your env file}
```

[jupyterconnect-mac.sh](./jupyterconnect-mac.sh) script opens a
jupyter notebook inside a Chrome web browser on a MAC.
```
./jupyterconnect-mac.sh {your env file}
```

[jupyterconnect-linux.sh](./jupyterconnect-linux.sh) script opens a
jupyter notebook inside a Chrome web browser on a Linux workstation.
```
./jupyterconnect-linux.sh {your env file}
```

This opens up the Cloud Datalab notebook in Chrome web browser. Cloud datalab
runs on the master node of the Cloud Dataproc cluster.

Upload [notebooks](./notebooks/*.ipynb) into Cloud Datalab and run it.
From there, just follow the instructions in the notebooks!

First-time users can simply read the instructions and execute the cells.
As you become familiar with the system, feel free to experiment by editing
queries or code and seeing what happens. Any changes you make will be saved
to your copy of the notebooks themselves. This notebook will use and create
datasets (Files) in GCS bucket you specified earlier in myenv.sh
